{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chest predictions",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "eD2iaR5mO9cY",
        "outputId": "704455f2-61c8-491d-b3b4-9fc9a953c3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 124\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrYg0Hh7Zyrq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1381c95-043b-4b96-d1a8-996836b56d13"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import cv2 \n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "x_train = []\n",
        "x_test = []\n",
        "# Dummy dataset\n",
        "data = np.load('/content/drive/MyDrive/Data/chestmnist.npz', allow_pickle=True)\n",
        "# print(data.files)\n",
        "# print(data['test_labels'])\n",
        "x=data['train_images']\n",
        "y=data['train_labels']\n",
        "\n",
        "#plt.scatter(x[:, 0], x[:, 1], c=y)\n",
        "print(x[0].shape)\n",
        "print(y.shape)\n",
        "print(\"The first sample : \", x[0],\"belong to cluster\", y[0])\n",
        "print(\"The tenth sample : \", x[10],\"belong to cluster\", y[10])\n",
        "\n",
        "\n",
        "# Split to a train set (80%), and a test set (20%)\n",
        "x_testPre, y_test = data[\"test_images\"], data[\"test_labels\"]\n",
        "x_trainPre, y_train = data[\"train_images\"], data[\"train_labels\"]\n",
        "# plt.gray()\n",
        "#\n",
        "\n",
        "img = x_trainPre[5].astype('float32')\n",
        "img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
        "plt.imshow(img)\n",
        "\n",
        "# for i in range(len(x_train)):\n",
        "#   x_train[i] = color_img = cv2.cvtColor(x_train[i], cv2.cv.CV_GRAY2RGB)\n",
        "# plt.imshow(x_train[1], interpolation='nearest')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n",
            "(78468, 14)\n",
            "The first sample :  [[ 61   9  13  12  12  12  13  19  31  45  70 103 123 143 162 162 162 143\n",
            "  117  97  64  40  21  13  11  13  21  22]\n",
            " [ 54   7  10  12  20  34  48  62  74  83  92 116 131 145 163 162 164 148\n",
            "  131 115  93  80  65  44  24  21  29  34]\n",
            " [ 64  23  46  59  72  87  98 105 105  95  87 103 126 147 166 161 166 152\n",
            "  128 107  90  91  97  86  66  45  27  15]\n",
            " [110  66 106 115 120 128 130 131 100  67  62  67  83 117 158 156 165 134\n",
            "   87  69  65  64  94 114 109  93  70  38]\n",
            " [144  92 124 148 152 141 132 115  84  69  63  57  68 101 143 153 159 104\n",
            "   66  57  60  60  82 125 133 128 114  93]\n",
            " [163 113 149 171 162 148 132  80  59  63  68  65  73 115 142 155 138  82\n",
            "   67  67  70  70  78 102 133 137 144 131]\n",
            " [173 128 158 177 164 153 118  63  51  48  58  71  84 134 155 163 128  82\n",
            "   70  66  54  52  63  90 140 148 163 167]\n",
            " [183 140 158 174 167 156 106  59  43  36  38  55  75 139 166 170 143  99\n",
            "   60  50  40  46  63  89 145 153 167 178]\n",
            " [189 139 147 165 165 155  96  49  41  38  38  51  78 144 170 169 148 105\n",
            "   69  48  40  46  58  89 151 160 175 178]\n",
            " [184 130 142 155 160 152  85  49  40  36  37  45  76 149 172 170 147 104\n",
            "   76  48  42  51  65  91 155 165 174 163]\n",
            " [179 128 142 151 155 150  81  42  35  36  43  50  90 157 176 177 149 107\n",
            "   77  52  42  47  65 101 159 166 162 150]\n",
            " [175 121 135 147 152 148  72  43  43  44  45  66 111 168 181 184 159 119\n",
            "   71  52  50  55  65 105 164 163 156 147]\n",
            " [164 104 119 142 152 143  73  48  44  44  50  74 114 169 181 187 164 129\n",
            "   89  55  50  59  71 113 171 166 159 144]\n",
            " [149  87 107 134 157 140  74  48  47  52  68  77 112 173 186 191 171 149\n",
            "  131  85  56  53  64 113 178 173 163 135]\n",
            " [129  54  83 111 155 133  73  56  56  62  76  80 115 177 190 195 175 157\n",
            "  148 126  84  61  67 104 181 179 166 130]\n",
            " [118  36  63  89 150 125  79  67  67  71  81  86 125 182 194 200 182 168\n",
            "  165 154 129  91  73 101 179 184 170 137]\n",
            " [108  29  56 100 157 127  86  76  77  84  87  92 135 186 197 205 188 174\n",
            "  171 163 151 123  87  98 173 191 176 146]\n",
            " [ 95  19  56 120 170 128  96 103 110 109 104 115 149 195 204 210 193 177\n",
            "  171 162 151 130 108 105 163 195 182 154]\n",
            " [ 77  10  54 141 178 130 133 148 153 158 158 157 169 200 208 213 195 177\n",
            "  172 160 145 131 118 111 150 198 187 162]\n",
            " [ 56  10  80 164 182 149 160 165 173 181 185 184 187 207 213 216 200 184\n",
            "  182 177 168 163 149 133 144 195 192 170]\n",
            " [ 50  28 122 173 185 166 172 181 189 195 198 200 207 218 220 222 213 204\n",
            "  204 202 199 195 185 173 158 190 194 175]\n",
            " [ 51  46 136 176 189 178 187 195 201 205 208 210 215 223 223 224 220 214\n",
            "  214 213 211 208 202 196 184 194 195 176]\n",
            " [ 52  38 134 175 195 194 201 206 210 213 216 216 220 226 227 227 224 220\n",
            "  220 219 217 216 214 209 203 207 198 176]\n",
            " [ 54  17 114 173 200 207 211 214 217 219 221 222 224 227 227 227 227 224\n",
            "  224 224 223 222 220 218 214 216 202 178]\n",
            " [ 54  10  93 168 200 214 218 220 222 224 226 226 225 229 230 229 227 225\n",
            "  225 227 226 223 222 222 221 220 206 177]\n",
            " [ 55   5  58 149 193 213 220 223 225 227 227 227 226 229 231 231 230 226\n",
            "  227 229 227 226 223 224 224 221 206 167]\n",
            " [ 57   4  32 114 174 208 218 224 227 228 228 229 228 231 232 231 232 228\n",
            "  229 229 230 228 227 226 224 221 204 146]\n",
            " [ 64   7  48 128 177 209 222 229 232 234 234 235 234 236 237 238 238 236\n",
            "  236 237 236 235 235 235 231 230 215 161]] belong to cluster [0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "The tenth sample :  [[ 13  10   9   8   8  16  43  70  84  92 112 132 149 164 161 162 144 128\n",
            "  107 100  99  86  63  34  18  24  37  26]\n",
            " [  5   3   3   9  27  52  71  81  81  81  88 102 126 137 130 136 109  90\n",
            "   84  88  92  89  77  59  39  38  32   2]\n",
            " [  3   7  34  64  86 103 110 104  74  57  49  66 105 131 125 126  81  54\n",
            "   54  67  91 115 118 112 112 119  65   9]\n",
            " [  9  53 111 133 133 142 136 102  64  52  40  50  80 119 124 116  67  47\n",
            "   50  57  72 115 145 147 149 137  96  49]\n",
            " [ 42  94 131 155 158 153 119  78  63  63  57  54  72 109 126 117  71  57\n",
            "   62  65  68  87 133 166 170 153 127  96]\n",
            " [ 85 126 152 172 169 155  99  66  48  47  58  63  73 104 131 123  75  64\n",
            "   56  45  48  67 111 166 179 169 148 127]\n",
            " [107 141 161 178 169 148  90  60  44  36  43  56  74 108 141 138  80  50\n",
            "   44  40  48  61  93 155 180 179 159 144]\n",
            " [117 148 166 179 170 141  84  60  48  40  40  53  71 114 150 155 104  50\n",
            "   38  35  40  52  83 142 179 179 165 154]\n",
            " [125 152 164 175 169 130  76  52  42  38  39  48  65 123 155 156 106  49\n",
            "   44  42  44  55  73 128 178 176 167 162]\n",
            " [132 155 163 172 165 115  73  60  46  40  44  57  75 129 159 156 119  73\n",
            "   58  39  44  59  76 117 174 174 170 170]\n",
            " [143 159 163 168 163 111  72  55  44  36  44  66  88 139 167 160 124  76\n",
            "   64  46  46  52  70 114 169 171 171 175]\n",
            " [153 158 161 160 162 106  69  58  54  50  59  83 104 155 173 168 136  80\n",
            "   69  57  55  60  74 105 163 164 166 173]\n",
            " [153 149 153 156 155 104  78  63  51  51  70  89 109 161 178 175 151  97\n",
            "   65  58  56  63  80 105 158 161 158 164]\n",
            " [151 147 151 157 153 103  77  65  60  67  85  91 113 165 181 182 165 128\n",
            "   91  68  64  69  78 103 156 162 158 164]\n",
            " [144 147 149 157 146  97  81  70  69  77  88  97 125 173 186 188 175 145\n",
            "  124  91  72  73  83  99 151 161 158 166]\n",
            " [137 149 146 158 135  97  81  73  73  80  94 109 135 180 190 194 184 163\n",
            "  150 123  87  77  87 101 141 158 153 166]\n",
            " [137 146 141 161 130  97  79  76  81  92 103 119 155 186 194 198 193 176\n",
            "  167 153 129 104  90 102 135 161 146 160]\n",
            " [131 136 141 165 124  95  84  78  78  85  95 114 159 191 197 203 194 179\n",
            "  171 165 158 139 105 103 129 165 142 148]\n",
            " [116 129 145 163 117  94  91  91  88  92 103 128 169 194 197 206 197 179\n",
            "  172 166 160 155 126 101 120 161 135 133]\n",
            " [ 91 115 146 157 117 118 133 141 139 137 137 150 176 195 197 208 197 179\n",
            "  173 166 160 154 134 106 110 150 119  84]\n",
            " [ 48  82 137 152 142 160 166 168 168 170 169 167 180 195 197 208 195 183\n",
            "  183 179 175 172 164 144 123 141  94  22]\n",
            " [ 18  29 115 161 174 182 184 185 187 192 193 191 192 199 199 206 201 203\n",
            "  197 182 179 185 190 184 166 142  77  20]\n",
            " [ 13  12  83 163 187 192 196 199 201 204 206 206 204 208 208 210 207 211\n",
            "  207 188 178 185 198 198 176 140  74  20]\n",
            " [  8  11  59 139 186 200 205 208 208 210 213 211 211 216 216 216 213 211\n",
            "  213 206 197 199 201 189 162 133  67  11]\n",
            " [  6   9  51 112 158 189 207 212 216 217 218 218 220 222 220 224 221 219\n",
            "  218 216 211 206 195 181 159 127  61   9]\n",
            " [  5   7  48 106 146 175 196 209 217 220 223 224 225 227 227 229 226 225\n",
            "  225 222 217 209 195 181 156 121  61  10]\n",
            " [  5   7  51 108 149 178 198 211 220 226 227 227 228 231 231 232 230 231\n",
            "  230 227 222 213 199 183 158 128  74  18]\n",
            " [  5  10  64 120 161 185 204 219 227 231 234 234 235 236 234 238 236 236\n",
            "  235 232 229 220 211 193 170 147  96  38]] belong to cluster [0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fddbcf6c610>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#img = x_train[5].astype('float32')\\\n",
        "x_train = []\n",
        "x_test = []\n",
        "for i in range(len(x_trainPre)):\n",
        "  var = cv2.cvtColor(x_trainPre[i],cv2.COLOR_GRAY2RGB)\n",
        "  var = np.transpose(var, [2, 0, 1])\n",
        "  x_train.append(var)\n",
        "for i in range(len(x_testPre)):\n",
        "  var = cv2.cvtColor(x_testPre[i],cv2.COLOR_GRAY2RGB)\n",
        "  var = np.transpose(var, [2, 0, 1])\n",
        "  x_test.append(var)\n",
        "print(x_train[0].shape)\n",
        "print(x_test[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEr4-6yYJ-4Y",
        "outputId": "4dcda39c-cb62-4dae-8cc3-dd81fe817a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 28, 28)\n",
            "(3, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "print(x_train[500].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeagGGQi-6fO",
        "outputId": "e0472c3e-ddae-40a7-f19f-c215b252700d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chestmnist.npz\tdrive  model2.pt  model3.pt  model.pt  sample_data\n",
            "(3, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = models.resnet18(pretrained=True).cuda()\n",
        "net.fc = nn.Linear(512, 14)\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFvmMPOsaJ_H",
        "outputId": "8ce78afb-efad-45ec-cf38-cc84dd1150b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=14, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "      super().__init__()\n",
        "      self.x = x\n",
        "      self.y = y \n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]\n",
        "\n",
        "  def __repr__(self):\n",
        "      return \"This is my dataset and it has length {}\".format(len(self))\n",
        "\n",
        "  def __len__(self):\n",
        "    #assert len(self.x) == len(self.y)\n",
        "    return len(self.y)"
      ],
      "metadata": {
        "id": "gs22XNorsgMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "my_dataset = MyDataset(x_train,y_train)\n",
        "print(my_dataset.__getitem__(0)[0].shape)\n",
        "my_dataloader = DataLoader(my_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
        "\n",
        "# Loss function (cross entropy for classification)\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training time !\n",
        "net.cuda()\n",
        "net.train() # Put model in training mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "808ZSlFXuJa5",
        "outputId": "79c31943-b959-45ad-e22f-0f9b51c04db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 28, 28)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=14, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.cuda()\n",
        "net.train() # Put model in training mode\n",
        "\n",
        "count = 0\n",
        "prev_loss = 0\n",
        "learning_rate = 0.1\n",
        "for epoch in range(100): # We go over the data ten times\n",
        "  loss_list = []\n",
        "  prev_loss = mean\n",
        "  mean = 0\n",
        "  sum = 0\n",
        "  for batch in my_dataloader:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass \n",
        "    inp, labels = batch\n",
        "    inp = torch.tensor(inp.cuda(), dtype=torch.float32)\n",
        "    labels = labels.cuda()\n",
        "    out = net(inp.cuda())\n",
        "    loss = loss_func(out, labels.float())\n",
        "    loss_list.append(loss)\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  for i in loss_list:\n",
        "    sum +=i\n",
        "  mean = sum/len(loss_list)\n",
        "  if count >= 3 and mean > prev_loss:\n",
        "    learning_rate = learning_rate/2\n",
        "    count = 0\n",
        "    for g in optimizer.param_groups:\n",
        "      g['lr'] = learning_rate\n",
        "  elif mean < prev_loss:\n",
        "    count+=1\n",
        "  else:\n",
        "    count = 0\n",
        "  print(f\"{epoch}: {mean}\", end = \"\")\n",
        "  print(f\", learning rate: {learning_rate}\", end = \"\")\n",
        "  print(f\", count: {count}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ1xRv2J03ay",
        "outputId": "e8e56826-ab42-4928-9c18-489ff17aff75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 0.0035071030724793673, learning rate: 0.1, count: 0\n",
            "1: 0.008172988891601562, learning rate: 0.1, count: 0\n",
            "2: 0.005914698354899883, learning rate: 0.1, count: 1\n",
            "3: 0.00472993403673172, learning rate: 0.1, count: 2\n",
            "4: 0.005165970418602228, learning rate: 0.1, count: 0\n",
            "5: 0.014243216253817081, learning rate: 0.1, count: 0\n",
            "6: 0.004403353668749332, learning rate: 0.1, count: 1\n",
            "7: 0.004969418980181217, learning rate: 0.1, count: 0\n",
            "8: 0.00526190921664238, learning rate: 0.1, count: 0\n",
            "9: 0.004152076784521341, learning rate: 0.1, count: 1\n",
            "10: 0.004254759754985571, learning rate: 0.1, count: 0\n",
            "11: 0.005995491053909063, learning rate: 0.1, count: 0\n",
            "12: 0.004730778746306896, learning rate: 0.1, count: 1\n",
            "13: 0.0054897116497159, learning rate: 0.1, count: 0\n",
            "14: 0.004483627621084452, learning rate: 0.1, count: 1\n",
            "15: 0.005414268933236599, learning rate: 0.1, count: 0\n",
            "16: 0.005147813353687525, learning rate: 0.1, count: 1\n",
            "17: 0.009701072238385677, learning rate: 0.1, count: 0\n",
            "18: 0.005180812440812588, learning rate: 0.1, count: 1\n",
            "19: 0.006166362203657627, learning rate: 0.1, count: 0\n",
            "20: 0.004355957265943289, learning rate: 0.1, count: 1\n",
            "21: 0.004504811018705368, learning rate: 0.1, count: 0\n",
            "22: 0.008728709071874619, learning rate: 0.1, count: 0\n",
            "23: 0.006150939967483282, learning rate: 0.1, count: 1\n",
            "24: 0.0036820920649915934, learning rate: 0.1, count: 2\n",
            "25: 0.0038337691221386194, learning rate: 0.1, count: 0\n",
            "26: 0.0061782593838870525, learning rate: 0.1, count: 0\n",
            "27: 0.003899999661371112, learning rate: 0.1, count: 1\n",
            "28: 0.0055252364836633205, learning rate: 0.1, count: 0\n",
            "29: 0.005425425246357918, learning rate: 0.1, count: 1\n",
            "30: 0.0031225515995174646, learning rate: 0.1, count: 2\n",
            "31: 0.003314909292384982, learning rate: 0.1, count: 0\n",
            "32: 0.004477088805288076, learning rate: 0.1, count: 0\n",
            "33: 0.004290053620934486, learning rate: 0.1, count: 1\n",
            "34: 0.005065394099801779, learning rate: 0.1, count: 0\n",
            "35: 0.0030330123845487833, learning rate: 0.1, count: 1\n",
            "36: 0.002346958965063095, learning rate: 0.1, count: 2\n",
            "37: 0.004309746436774731, learning rate: 0.1, count: 0\n",
            "38: 0.005771115887910128, learning rate: 0.1, count: 0\n",
            "39: 0.005342911928892136, learning rate: 0.1, count: 1\n",
            "40: 0.006850389298051596, learning rate: 0.1, count: 0\n",
            "41: 0.002780822105705738, learning rate: 0.1, count: 1\n",
            "42: 0.004718490410596132, learning rate: 0.1, count: 0\n",
            "43: 0.006890591233968735, learning rate: 0.1, count: 0\n",
            "44: 0.005469531752169132, learning rate: 0.1, count: 1\n",
            "45: 0.006347961723804474, learning rate: 0.1, count: 0\n",
            "46: 0.004849235061556101, learning rate: 0.1, count: 1\n",
            "47: 0.009743759408593178, learning rate: 0.1, count: 0\n",
            "48: 0.007825990207493305, learning rate: 0.1, count: 1\n",
            "49: 0.0038491743616759777, learning rate: 0.1, count: 2\n",
            "50: 0.009775196202099323, learning rate: 0.1, count: 0\n",
            "51: 0.011379406787455082, learning rate: 0.1, count: 0\n",
            "52: 0.007319911848753691, learning rate: 0.1, count: 1\n",
            "53: 0.004698265343904495, learning rate: 0.1, count: 2\n",
            "54: 0.004563508089631796, learning rate: 0.1, count: 3\n",
            "55: 0.004601657390594482, learning rate: 0.05, count: 0\n",
            "56: 0.00319648627191782, learning rate: 0.05, count: 1\n",
            "57: 0.0024796007201075554, learning rate: 0.05, count: 2\n",
            "58: 0.004124049097299576, learning rate: 0.05, count: 0\n",
            "59: 0.004984679631888866, learning rate: 0.05, count: 0\n",
            "60: 0.003881683573126793, learning rate: 0.05, count: 1\n",
            "61: 0.004602585453540087, learning rate: 0.05, count: 0\n",
            "62: 0.003576587187126279, learning rate: 0.05, count: 1\n",
            "63: 0.004029826261103153, learning rate: 0.05, count: 0\n",
            "64: 0.003242808161303401, learning rate: 0.05, count: 1\n",
            "65: 0.004357697907835245, learning rate: 0.05, count: 0\n",
            "66: 0.0030638433527201414, learning rate: 0.05, count: 1\n",
            "67: 0.0021662372164428234, learning rate: 0.05, count: 2\n",
            "68: 0.0035727114882320166, learning rate: 0.05, count: 0\n",
            "69: 0.0035048294812440872, learning rate: 0.05, count: 1\n",
            "70: 0.0049466537311673164, learning rate: 0.05, count: 0\n",
            "71: 0.004451744724065065, learning rate: 0.05, count: 1\n",
            "72: 0.00442469073459506, learning rate: 0.05, count: 2\n",
            "73: 0.006234646309167147, learning rate: 0.05, count: 0\n",
            "74: 0.005827850662171841, learning rate: 0.05, count: 1\n",
            "75: 0.00481512863188982, learning rate: 0.05, count: 2\n",
            "76: 0.00933072715997696, learning rate: 0.05, count: 0\n",
            "77: 0.0040337094105780125, learning rate: 0.05, count: 1\n",
            "78: 0.003990232013165951, learning rate: 0.05, count: 2\n",
            "79: 0.0037023143377155066, learning rate: 0.05, count: 3\n",
            "80: 0.00493023544549942, learning rate: 0.025, count: 0\n",
            "81: 0.0033176513388752937, learning rate: 0.025, count: 1\n",
            "82: 0.0025657101068645716, learning rate: 0.025, count: 2\n",
            "83: 0.0020895288325846195, learning rate: 0.025, count: 3\n",
            "84: 0.002570341108366847, learning rate: 0.0125, count: 0\n",
            "85: 0.0028511835262179375, learning rate: 0.0125, count: 0\n",
            "86: 0.002555007115006447, learning rate: 0.0125, count: 1\n",
            "87: 0.002809039782732725, learning rate: 0.0125, count: 0\n",
            "88: 0.002709784312173724, learning rate: 0.0125, count: 1\n",
            "89: 0.0023600165732204914, learning rate: 0.0125, count: 2\n",
            "90: 0.0020931328181177378, learning rate: 0.0125, count: 3\n",
            "91: 0.0023914615157991648, learning rate: 0.00625, count: 0\n",
            "92: 0.00212293048389256, learning rate: 0.00625, count: 1\n",
            "93: 0.002125832950696349, learning rate: 0.00625, count: 0\n",
            "94: 0.0016986699774861336, learning rate: 0.00625, count: 1\n",
            "95: 0.0019194346386939287, learning rate: 0.00625, count: 0\n",
            "96: 0.0015153756830841303, learning rate: 0.00625, count: 1\n",
            "97: 0.0016561284428462386, learning rate: 0.00625, count: 0\n",
            "98: 0.0015843535074964166, learning rate: 0.00625, count: 1\n",
            "99: 0.0017398287309333682, learning rate: 0.00625, count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net, 'model4.pt')"
      ],
      "metadata": {
        "id": "8_BKmMNEL2jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = torch.load('model.pt')"
      ],
      "metadata": {
        "id": "WaYnsXxE4MeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H1wSuZbl4MPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test time\n",
        "\n",
        "net.eval()\n",
        "\n",
        "# Dataloader\n",
        "my_dataset = MyDataset(x_test,y_test)\n",
        "my_dataloader = DataLoader(my_dataset, batch_size=16)\n",
        "\n",
        "network_answers = []\n",
        "true_answers = []\n",
        "for batch in my_dataloader:\n",
        "  # Forward pass \n",
        "  inp, labels = batch\n",
        "  inp = torch.tensor(inp.cuda(), dtype=torch.float32)\n",
        "  out = net(inp)\n",
        "  # print(out)\n",
        "\n",
        "  # out is batch_size x 2 (one score for each cluster)\n",
        "  sigmoid_layer = torch.nn.Sigmoid()\n",
        "  answers = sigmoid_layer(out).cpu().detach().numpy()\n",
        "\n",
        "  # Recording values\n",
        "\n",
        "  preds = answers >= 0.5\n",
        "\n",
        "  network_answers.extend(np.asarray(preds))\n",
        "  true_answers.extend(labels.data.cpu().numpy())\n",
        "\n",
        "\n",
        "print(f\" Accuracy Score: {accuracy_score(true_answers, network_answers)}\")\n",
        "print(f\" Precision Score: {precision_score(true_answers, network_answers, average='micro')}\")\n",
        "\n",
        "# try to test the machine before training, how much is the accuracy ?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxWsWlMD6TKp",
        "outputId": "c9d79b9d-1d21-4f11-b9cb-c272e5e6ce20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy Score: 0.44991753220701647\n",
            " Precision Score: 0.2575117953811771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEvxt-dZmkAZ",
        "outputId": "59883b4c-852f-429b-b81b-55a278eb0a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False  True False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False  True False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[ True False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False  True False False False False\n",
            " False False]\n",
            "[False False False False  True False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False  True False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False  True False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False  True False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False  True False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False  True False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n",
            "[False False False False False False False False False False False False\n",
            " False False]\n"
          ]
        }
      ]
    }
  ]
}